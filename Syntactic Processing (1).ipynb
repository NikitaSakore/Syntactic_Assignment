{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f3f679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in e:\\lk\\lib\\site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: setuptools in e:\\lk\\lib\\site-packages (61.2.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: wheel in e:\\lk\\lib\\site-packages (0.37.1)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.37.1\n",
      "    Uninstalling wheel-0.37.1:\n",
      "      Successfully uninstalled wheel-0.37.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 61.2.0\n",
      "    Uninstalling setuptools-61.2.0:\n",
      "      Successfully uninstalled setuptools-61.2.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-22.3.1 setuptools-65.5.1 wheel-0.38.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in e:\\lk\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: jinja2 in e:\\lk\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\lk\\lib\\site-packages (from spacy) (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\lk\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\lk\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\lk\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\lk\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\lk\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: setuptools in e:\\lk\\lib\\site-packages (from spacy) (65.5.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\lk\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\lk\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\lk\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\lk\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\lk\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\lk\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\lk\\lib\\site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\lk\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\lk\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\lk\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\lk\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\lk\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in e:\\lk\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in e:\\lk\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\lk\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\lk\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: colorama in e:\\lk\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\lk\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\lk\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in e:\\lk\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: setuptools in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (65.5.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.27.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: jinja2 in e:\\lk\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\lk\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in e:\\lk\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in e:\\lk\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\lk\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\lk\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: colorama in e:\\lk\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\lk\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\lk\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "\n",
    "!pip install -U spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7ae7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycrf in e:\\lk\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in e:\\lk\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in e:\\lk\\lib\\site-packages (from sklearn-crfsuite) (0.9.8)\n",
      "Requirement already satisfied: tabulate in e:\\lk\\lib\\site-packages (from sklearn-crfsuite) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=2.0 in e:\\lk\\lib\\site-packages (from sklearn-crfsuite) (4.64.0)\n",
      "Requirement already satisfied: six in e:\\lk\\lib\\site-packages (from sklearn-crfsuite) (1.16.0)\n",
      "Requirement already satisfied: colorama in e:\\lk\\lib\\site-packages (from tqdm>=2.0->sklearn-crfsuite) (0.4.4)\n",
      "Requirement already satisfied: spacy in e:\\lk\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in e:\\lk\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\lk\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\lk\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\lk\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: setuptools in e:\\lk\\lib\\site-packages (from spacy) (65.5.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\lk\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in e:\\lk\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\lk\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in e:\\lk\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\lk\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\lk\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\lk\\lib\\site-packages (from spacy) (0.7.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\lk\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in e:\\lk\\lib\\site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\lk\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in e:\\lk\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\lk\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\lk\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in e:\\lk\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\lk\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in e:\\lk\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in e:\\lk\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in e:\\lk\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\lk\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\lk\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: colorama in e:\\lk\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in e:\\lk\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\lk\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycrf\n",
    "!pip install sklearn-crfsuite\n",
    "!pip install spacy\n",
    "\n",
    "\n",
    "import spacy\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "\n",
    "model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d538c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting words to sentence\n",
    "def tokens_to_sentence(input_f):\n",
    "    file_line = input_f.readlines()\n",
    "    input_f.close()\n",
    "    final_sentence_list = []\n",
    "\n",
    "    sentence = \"\"\n",
    "\n",
    "    for sent in file_line:\n",
    "        sent = sent.strip()\n",
    "        \n",
    "        if sent == \"\":\n",
    "            final_sentence_list.append(sentence) \n",
    "            sentence = \"\" \n",
    "        else:\n",
    "            if sentence:\n",
    "                sentence += \" \" + sent\n",
    "            else:\n",
    "                sentence = sent\n",
    "                \n",
    "    return final_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6f5c09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aniket Kale\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fff87621",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading input files\n",
    "train_sent = open('train_sent','r')\n",
    "train_sentence=tokens_to_sentence(train_sent)\n",
    "test_sent=open('test_sent','r')\n",
    "test_sentence=tokens_to_sentence(test_sent)\n",
    "train_label=open('train_label','r')\n",
    "train_label=tokens_to_sentence(train_label)\n",
    "test_label=open('test_label','r')\n",
    "test_label=tokens_to_sentence(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc2c6b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1 : All live births > or = 23 weeks at the University of Vermont in 1995 ( n = 2395 ) were retrospectively analyzed for delivery route , indication for cesarean , gestational age , parity , and practice group ( to reflect risk status )\n",
      "Label 1 : O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Line 2 : The total cesarean rate was 14.4 % ( 344 of 2395 ) , and the primary rate was 11.4 % ( 244 of 2144 )\n",
      "Label 2 : O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Line 3 : Abnormal presentation was the most common indication ( 25.6 % , 88 of 344 )\n",
      "Label 3 : O O O O O O O O O O O O O O O\n",
      "Line 4 : The `` corrected '' cesarean rate ( maternal-fetal medicine and transported patients excluded ) was 12.4 % ( 273 of 2194 ) , and the `` corrected '' primary rate was 9.6 % ( 190 of 1975 )\n",
      "Label 4 : O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Line 5 : Arrest of dilation was the most common indication in both `` corrected '' subgroups ( 23.4 and 24.6 % , respectively )\n",
      "Label 5 : O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "## First 5 sentence of the Train dataset\n",
    "for i in range(0,5):\n",
    "    print('Line %d' %(i+1) ,':',train_sentence[i])\n",
    "    print('Label %d' %(i+1),':',train_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9ecca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1 : Furthermore , when all deliveries were analyzed , regardless of risk status but limited to gestational age > or = 36 weeks , the rates did not change ( 12.6 % , 280 of 2214 ; primary 9.2 % , 183 of 1994 )\n",
      "Label 1 : O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Line 2 : As the ambient temperature increases , there is an increase in insensible fluid loss and the potential for dehydration\n",
      "Label 2 : O O O O O O O O O O O O O O O O O O O\n",
      "Line 3 : The daily high temperature ranged from 71 to 104 degrees F and AFI values ranged from 1.7 to 24.7 cm during the study period\n",
      "Label 3 : O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Line 4 : There was a significant correlation between the 2- , 3- , and 4-day mean temperature and AFI , with the 4-day mean being the most significant ( r = 0.31 , p & # 60 ; 0.001 )\n",
      "Label 4 : O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Line 5 : Fluctuations in ambient temperature are inversely correlated to changes in AFI\n",
      "Label 5 : O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "## First 5 sentence of the Test dataset\n",
    "for i in range(0,5):\n",
    "    print('Line %d' %(i+1) ,':',test_sentence[i])\n",
    "    print('Label %d' %(i+1),':',test_label[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edde41a",
   "metadata": {},
   "source": [
    "Count the number of sentences in the processed train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "338f3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of lines in train dataset is:  2599\n",
      "No of lines in test dataset is:  1056\n"
     ]
    }
   ],
   "source": [
    "## Total number of sentences in train & test dataset\n",
    "print(\"No of lines in train dataset is: \",len(train_sentence))\n",
    "print(\"No of lines in test dataset is: \",len(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486f39f",
   "metadata": {},
   "source": [
    "Count the number of lines of labels in the processed train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80171a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of lines in train dataset is:  2599\n",
      "No of lines in test dataset is:  1056\n"
     ]
    }
   ],
   "source": [
    "## Total number of labels in train & test dataset\n",
    "print(\"No of lines in train dataset is: \",len(train_label))\n",
    "print(\"No of lines in test dataset is: \",len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce04a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1e531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85af3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66cb3b2a",
   "metadata": {},
   "source": [
    "Concept Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a02aa97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will first explore what are the various concepts present in the dataset. For this, we will use PoS Tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "479838a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract those tokens which have NOUN or PROPN as their PoS tag and find their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79e811aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing frequency distribution\n",
    "from nltk import FreqDist\n",
    "\n",
    "## Method for extracting if the POS tag is a NOUN or PROPN\n",
    "def extractNounPropn(train_sentence,test_sentence):\n",
    "    train_test_POS=[]\n",
    "    for sentn in (train_sentence,test_sentence):\n",
    "        for sent in sentn:\n",
    "            for word in model(sent):\n",
    "                if(word.pos_=='NOUN' or word.pos_=='PROPN'):\n",
    "                    train_test_POS.append(word.text)\n",
    "    return train_test_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4ba662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['births',\n",
       " 'weeks',\n",
       " 'University',\n",
       " 'Vermont',\n",
       " 'delivery',\n",
       " 'route',\n",
       " 'indication',\n",
       " 'cesarean',\n",
       " 'age',\n",
       " 'parity']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Words with noun or proper noun as POS tag\n",
    "train_test_POS=extractNounPropn(train_sentence,test_sentence)\n",
    "train_test_POS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a6158ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the top 25 most common tokens with NOUN or PROPN PoS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b849d324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patients', 492),\n",
       " ('treatment', 281),\n",
       " ('%', 247),\n",
       " ('cancer', 200),\n",
       " ('therapy', 175),\n",
       " ('study', 154),\n",
       " ('disease', 142),\n",
       " ('cell', 140),\n",
       " ('lung', 116),\n",
       " ('group', 94),\n",
       " ('chemotherapy', 88),\n",
       " ('gene', 87),\n",
       " ('effects', 85),\n",
       " ('women', 77),\n",
       " ('results', 77),\n",
       " ('use', 75),\n",
       " ('risk', 71),\n",
       " ('cases', 71),\n",
       " ('surgery', 71),\n",
       " ('analysis', 70),\n",
       " ('rate', 67),\n",
       " ('dose', 66),\n",
       " ('response', 66),\n",
       " ('survival', 65),\n",
       " ('children', 64)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(train_test_POS).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4a960",
   "metadata": {},
   "source": [
    "Defining features for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d41f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the features to get the feature value for one word.\n",
    "def featuresForOneWord(sentence,original_sent,pos):\n",
    "    word=sentence[pos]\n",
    "    tag_word=model(original_sent)\n",
    "    features=['word.lower=' + word.lower(),\n",
    "              'word[-3:]=' + word[-3:],\n",
    "              'word[-2:]=' + word[-2:],\n",
    "              'word.isupper=%s' %word.isupper(),\n",
    "              'word.isdigit=%s' %word.isdigit(),\n",
    "              'word.startsWithCapital=%s' %word[0].isupper(),\n",
    "              'tag_word.pos_=%s' %tag_word[pos].pos_]\n",
    "    if(pos>0):\n",
    "        prev_word=sentence[pos-1]\n",
    "        features.extend(['prev_word.isupper=%s' %prev_word.isupper(),\n",
    "                         'prev_word.lower=' + prev_word.lower(),\n",
    "                         'prev_word.isdigit=%s' %prev_word.isdigit(),\n",
    "                         'prev_word.startsWithCapital=%s' %prev_word[0].isupper(),\n",
    "                         'tag_prevword.pos_=%s'%tag_word[pos-1].pos_])\n",
    "    else:\n",
    "        features.append('BEG')\n",
    "    if(pos==len(sentence)-1):\n",
    "        features.append('END')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67eb3a",
   "metadata": {},
   "source": [
    "Getting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d37c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a code to get features for a sentence.\n",
    "def featuresForOneSentence(sentence):\n",
    "    sentence_split=sentence.split()\n",
    "    return [featuresForOneWord(sentence_split,sentence,pos) for pos in range(len(sentence_split))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b6cb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a code to get the labels for a sentence.\n",
    "def labelsForSentence(labels):\n",
    "    return labels.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2da9f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the features' values for each sentence as input variable for CRF model in test and the train dataset\n",
    "\n",
    "X_train=[featuresForOneSentence(sentn) for sentn in train_sentence]\n",
    "X_test=[featuresForOneSentence(sentn) for sentn in test_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fd598ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the labels as the target variable for test and the train dataset\n",
    "\n",
    "Y_train=[labelsForSentence(labels) for labels in train_label]\n",
    "Y_test=[labelsForSentence(labels) for labels in test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1634013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CRF model.\n",
    "try:\n",
    "    crf=sklearn_crfsuite.CRF(max_iterations=100)\n",
    "    crf.fit(X_train,Y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b36fdc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the labels of each of the tokens in each sentence of the test dataset that has been pre processed earlier.\n",
    "\n",
    "Y_pred=crf.predict(X_test)\n",
    "Y_pred[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60b5a49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score : 0.9042102390347845\n"
     ]
    }
   ],
   "source": [
    "#Calculate the f1 score using the actual labels and the predicted labels of the test dataset.\n",
    "\n",
    "print(\"f1 score :\",metrics.flat_f1_score(Y_test,Y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2f4ea",
   "metadata": {},
   "source": [
    "Identifying Diseases and Treatments using Custom NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcd5b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Disease_Treatment_dict=dict()\n",
    "\n",
    "for lbl in range(len(Y_pred)):\n",
    "    disease=\"\"\n",
    "    treatment=\"\"\n",
    "    label=Y_pred[lbl]\n",
    "    for j in range(len(label)):\n",
    "        if label[j]=='D':\n",
    "            disease=disease+test_sentence[lbl].split()[j]+\" \"\n",
    "        if label[j]=='T':\n",
    "            treatment=treatment+test_sentence[lbl].split()[j]+\" \"\n",
    "    disease=disease.lstrip().rstrip()\n",
    "    treatment=treatment.lstrip().rstrip()\n",
    "    if disease!='' and treatment!='':\n",
    "        if disease in Disease_Treatment_dict.keys():\n",
    "            existing_disease_treatment=list(Disease_Treatment_dict[disease]) \n",
    "            existing_disease_treatment.append(treatment)\n",
    "            Disease_Treatment_dict[disease]=existing_disease_treatment\n",
    "        else:\n",
    "            Disease_Treatment_dict[disease]=treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b8d7a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'radiotherapy'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Disease_Treatment_dict['hereditary retinoblastoma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9a160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
